{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767141f7",
   "metadata": {},
   "source": [
    "## Imports and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f535bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c585cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "212bb6cf",
   "metadata": {},
   "source": [
    "## Create a corpus from all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53f300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads in data from all json files\n",
    "\n",
    "src = \"C:\\\\Users\\\\theso\\\\semeruDatasets\"\n",
    "\n",
    "arrOfData = []\n",
    "\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    src_files_2 = os.listdir(full_file_name)\n",
    "    for json_name in src_files_2:\n",
    "        json_file_name = os.path.join(full_file_name, json_name)\n",
    "        if os.path.isfile(json_file_name):\n",
    "            arrOfData += [json.load(open(json_file_name, encoding=\"utf-8\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dae4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for i in range(len(arrOfData)):\n",
    "    #print(i / len(arrOfData) * 100)\n",
    "    for j in range(len(arrOfData[i])):\n",
    "        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(arrOfData[i][j][\"code\"])\n",
    "        new_words = [word for word, offset in words_with_offsets]\n",
    "        for word in new_words:\n",
    "            word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77566ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ï', 'ð', 'ĉ', 'Ċ', 'Č', 'č', 'Ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń']\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec762cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac233f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0dbf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 'Ġ') 47819588\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "\n",
    "#finding best pairs\n",
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "        \n",
    "merges = {}\n",
    "        \n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d678f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges pairs\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "#finds most frequent pairs and adds them to splits\n",
    "def makePairs(splits, vSize):    \n",
    "    vocab_size = vSize\n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_freqs = compute_pair_freqs(splits)\n",
    "        best_pair = \"\"\n",
    "        max_freq = None\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            if max_freq is None or max_freq < freq:\n",
    "                best_pair = pair\n",
    "                max_freq = freq\n",
    "        splits = merge_pair(*best_pair, splits)\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a091d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ï', 'ð', 'ĉ', 'Ċ', 'Č', 'č', 'Ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ĠĠ', 'ĠĠĠĠ', 'ĠĠĠĠĠĠĠĠ', 'ĠĠĠ', 'ĠĠĠĠĠĠĠ', 'se', 'ĊĠĠĠĠĠĠĠĠ', 'in', 'ĊĠĠĠĠĠĠĠ', 'on', 'at', 're', 'er', 'st', 'Ġ=', 'or', 'de', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'en', 'sel', 'al', 'self', 'ar', 'Ġt', 'as', 'ĊĠĠĠ', 'Ġ\"', 'it', 'an', \"Ġ'\", 'le', 'me', 'Ġself', 'ion', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'ct', 'Ġi', 'Ġc', 'ge', 'Ġ#', 'ce', 'ut', 'Ġf', 'ser', 'ro', 'ate', 'lo', 'he', '\",', 'ent', 'Ġre', 'ue', 'Ġp', 'ra', \"':\", 'Ġn', 'fi', 'id', '()', 'co', 'pe', 'ur', 'ex', 'ing', 'Ġin', '\":', 'li', 'mo', 'un', 'ch', 'ck', 'Ġs', 'di', 'ame', 'def', 'ad', '):', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'ot', 'sert', 'ed', 'assert', 'mp', 'ata', 'ul', '00', 'Ġa', 'est', 'Ġif', 'get', 'one', 'ri', 'con', 'ss', '--', 'Ġd', 'um', 'val', 'Ġw', 'la', 'el', '),', \"',\", 'ma', 'Ġ{', 'Ġb', 'Ġo', 'Ġ1', 'set', 'Ġ[', 'name', 'Ġ)', 'ti', 'up', 'None', 'ype', 'ĊĊĠĠĠĠĠĠĠ', '(\"', 'and', 'arg', '._', 'Ġ==', 'ect', 'qu', 'es', 'Ġassert', 'ation', 'ol', 'Ġis', 'et', 'ult', '\")', 'Ġfor', 'urn', '\"]', 'fig', 'der', 'turn', 'ĠNone', 'pt', 'Ġthe', 'iz', 'Ġ(', 'ĠT', 'ass', 'sp', 'ver', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'data', 'Ġm', 'Ġ0', 'il', 'ĊĊĠĠĠ', 'ith', 'Ġst', 'vi', 'ble', 'Ġreturn', 'mode', 'rue', 'int', '[\"', 'ext', 'Ġnot', 'put', 'to', 'args', 'Ġcon', 'ap', 'Ġh', 'po', 'is', 'ke', '],', 'value', 'type', 'Ġto', 'ath', '=\"', 'res', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'ror', 'nc', 'pro', 'ort', 'model', 'Ġtest', 'ig', 've', 'Ġ-', 'Ġ_', 'ction', 'Ġex', 'all', 'ter', 'test', 'tr', 'able', '))', 'Ġe', 'ho', 'Ġ}', 'par', 'for', '----', 'am']\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "#replce 414 with length of vocab\n",
    "makePairs(splits, 414)\n",
    "\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365adc8",
   "metadata": {},
   "source": [
    "## Deduplicate file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbd21c",
   "metadata": {},
   "source": [
    "#### manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "863beb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c2685e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call these functions with word freqs\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def jaccardSet(list1, list2):\n",
    "    list1 = set(list1)\n",
    "    list2 = set(list2)\n",
    "    \n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "251a6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isDupeHelper(data, i, j):\n",
    "    \n",
    "    #nvm this is broken, will only ever iterate a single time \n",
    "    if jaccard(data[i][\"BPE_tokens\"], data[j][\"BPE_tokens\"]) >= 0.7 and jaccardSet(data[i][\"BPE_tokens\"],data[j][\"BPE_tokens\"]) >= 0.8:\n",
    "        return True\n",
    "    else:\n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6516588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual dedupes\n",
    "#for code section\n",
    "src = \"C:\\\\Users\\\\theso\\\\semeruDatasets\\\\kivy\\\\data_1.json\"\n",
    "data = json.load(open(src))\n",
    "\n",
    "tokenSet = set()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    #compare each code sections to every code section following it\n",
    "    tokenSet.add(data[i][\"code\"])\n",
    "    data[i].update({\"BPE_tokens\": tokenize(data[i][\"code\"])})\n",
    "\n",
    "#for i in len(list): iterate through every item\n",
    "\n",
    "def Deduper(data):\n",
    "    \n",
    "    retSet = set()\n",
    "    \n",
    "    for i in range(len(data)): \n",
    "        dupe = False\n",
    "        for j in range(i, len(data)):\n",
    "            if isDupeHelper(data, i, j):\n",
    "                #print(str(jaccard(data[i][\"BPE_tokens\"], data[j][\"BPE_tokens\"])) + \" \" + str(jaccardSet(data[i][\"BPE_tokens\"],data[j][\"BPE_tokens\"]))) \n",
    "                retSet.add(data[i][\"code\"])\n",
    "                #print(retSet)\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aef1ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lset = Deduper(data)\n",
    "\n",
    "goodSet = tokenSet - Lset\n",
    "retFile = []\n",
    "\n",
    "for i in range(len(data)): \n",
    "    if data[i][\"code\"] in goodSet:\n",
    "        del data[i][\"BPE_tokens\"] #this is because python has an interpertation problem with the special characters used in BPE\n",
    "        retFile.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ccbde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_dedupe\", \"w\") as f:\n",
    "    json.dump(retFile, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a85a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size after removing duplicates: 288\n"
     ]
    }
   ],
   "source": [
    "print(\"size after removing duplicates: \" + str(len(retFile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e07cc",
   "metadata": {},
   "source": [
    "#### with tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81aa7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tool\n",
    "from dpu_utils.codeutils.deduplication import DuplicateDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "585f0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"C:\\\\Users\\\\theso\\\\semeruDatasets\\\\kivy\\\\data_1.json\"\n",
    "data = json.load(open(src))\n",
    "\n",
    "\n",
    "dupes = DuplicateDetector()\n",
    "\n",
    "\n",
    "def Deduper(data):\n",
    "    dupes = DuplicateDetector()\n",
    "\n",
    "    totalSet = set()\n",
    "          \n",
    "    for i in range(len(data)):\n",
    "\n",
    "        if data[i][\"code\"] not in totalSet:   \n",
    "            dupes.add_file(data[i][\"code\"], tokenize(data[i][\"code\"]))\n",
    "\n",
    "        totalSet.add(data[i][\"code\"])\n",
    "        \n",
    "    what = dupes.compute_duplicates()\n",
    "        \n",
    "    exclude = dupes.compute_ids_to_exclude()\n",
    "    \n",
    "    retFileTool = []\n",
    "\n",
    "    for i in range(len(data)): \n",
    "        if data[i][\"code\"] not in exclude: \n",
    "            retFileTool.append(data[i])\n",
    "            \n",
    "    return retFileTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6ae9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retFileTool = Deduper(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3027765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_dedupe\", \"w\") as f:\n",
    "    json.dump(retFile, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e34aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size after removing duplicates: 261\n"
     ]
    }
   ],
   "source": [
    "print(\"size after removing duplicates: \" + str(len(retFileTool)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0870a",
   "metadata": {},
   "source": [
    "## For the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa99668",
   "metadata": {},
   "source": [
    "What deduplicating the entire dataset with this tool might look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c7c2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"C:\\\\Users\\\\theso\\\\semeruDatasets\"\n",
    "\n",
    "src_files = os.listdir(src)\n",
    "\n",
    "dupes = DuplicateDetector()\n",
    "\n",
    "totalSet = set()\n",
    "\n",
    "\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    print(file_name)\n",
    "    src_files_2 = os.listdir(full_file_name)\n",
    "    for json_name in src_files_2:\n",
    "        json_file_name = os.path.join(full_file_name, json_name)\n",
    "        #print(json_file_name)\n",
    "        if os.path.isfile(json_file_name):\n",
    "            data = json.load(open(json_file_name, encoding=\"utf-8\"))\n",
    "            for i in range(len(data)):\n",
    "                print(i)\n",
    "                if data[i][\"code\"] not in totalSet:   \n",
    "                    dupes.add_file(data[i][\"code\"], tokenize(data[i][\"code\"]))\n",
    "                    \n",
    "exclude = dupes.compute_ids_to_exclude()\n",
    "    \n",
    "retFileTool = []\n",
    "\n",
    "for i in range(len(data)): \n",
    "    if data[i][\"code\"] not in exclude: \n",
    "        retFileTool.append(data[i])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235de894",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name + \"Deduped\", \"w\") as f:\n",
    "                json.dump(retFile, f, ensure_ascii=False, indent=4)\n",
    "            print(\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
